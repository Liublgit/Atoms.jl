{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Fitting functions\n",
    "\n",
    "* Noisy measurements: free energy derivatives\n",
    "* Kernel fitting\n",
    "* Gaussian process\n",
    "* Free energy surface reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "include(\"mdlecturesrc.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydrogen bonding in the water dimer\n",
    "\n",
    "The free energy is the quantity that tells us about the probability of occupying a certain macroscopic or \"mesoscopic\" state. E.g. as a function of O-O distance\n",
    "\n",
    "$$ \n",
    "A(r) = -kT \\ln P(r) = -kT \\ln \\int dq \\,\\delta(r-r_\\mathrm{OO}) \\, e^{-V(q)/kT}\n",
    "$$\n",
    "\n",
    "Fix one O atom at the origin, let $r$ be the $x$ coordinate of the other O atom. For a simple coordinate like this, \n",
    "\n",
    "$$\n",
    "\\nabla A(r) = \\langle \\nabla V \\rangle_{r=r_\\mathrm{OO}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imolecule_draw(make_h4o2(optim=true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r, e = water_dimer_dissoc()\n",
    "    \n",
    "figure()\n",
    "plot(r, e, \"k-\");\n",
    "xlabel(L\"r_\\mathrm{OO}\"); ylabel(\"Energy\"); title(\"Water dimer TIP3P\")\n",
    "for rc in [3,3.5,4,4.5,5,5.5,6]\n",
    "    plot([rc,rc], [-0.29,0.29], \"k--\")\n",
    "end\n",
    "close(gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = water_dimer_dynamics(;Temp=100.0, Nsteps=1000, Nsave=0,\n",
    "        fix_axes=true,constrainO2=true, OOdistance=3.0)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure()\n",
    "plot(f)\n",
    "xlabel(\"iteration\"); ylabel(\"force on constrained O\"); title(\"constrained water dimer T=100 K\")\n",
    "close(gcf())\n",
    "\n",
    "\n",
    "figure()\n",
    "plt[:hist](f,20)\n",
    "xlabel(\"force value\"); ylabel(\"count\")\n",
    "close(gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "mean_err_corr(f; acorr_limit=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rOO = []\n",
    "fmean = []\n",
    "ferr = []\n",
    "for r in (2.6:0.2:6.0)\n",
    "    f = water_dimer_dynamics(;Temp=200.0, Nsteps=1000, Nsave=0, fix_axes=true,\n",
    "                constrainO2=true, OOdistance=r)\n",
    "    fm,fe = mean_err_corr(f[500:end]; acorr_limit=20)\n",
    "    push!(fmean, fm)\n",
    "    push!(ferr, fe)\n",
    "    push!(rOO, r)\n",
    "    println(\"r=$r done\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure()\n",
    "errorbar(rOO, fmean, ferr)\n",
    "xlabel(L\"r_\\mathrm{OO}\"); ylabel(\"f\")\n",
    "close(gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure()\n",
    "a = cumsum(-fmean)*0.2\n",
    "plot(rOO, a-minimum(a), \"bo-\")\n",
    "xlabel(L\"r_\\mathrm{OO}\"); ylabel(\"A(r)\")\n",
    "close(gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function fitting with kernels in 1 dimension\n",
    "\n",
    "Suppose we want to fit the function $f(x)$, and we have $N$ observations $y_i$ at locations $x_i$\n",
    "\n",
    "We write the ansatz \n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^N \\alpha_i k(x_i, x)\n",
    "$$\n",
    "\n",
    "where $k$ is a _kernel_, which we use to construct basis functions, e.g. \n",
    "\n",
    "$$\n",
    "k(x, x') = \\sigma^2_w e^{-|x-x'|^2/2\\sigma^2}\n",
    "$$\n",
    "\n",
    "Because the ansatz is linear in its parameters $\\alpha$, finding them is easy, we substitute the data:\n",
    "\n",
    "$$\n",
    "y_j = \\sum_{i=1}^N \\alpha_i k(x_i, x_j)\n",
    "$$\n",
    "\n",
    "This leads to a linear problem that is typically not invertible, so we _regularise_ it by adding something to the diagonal,\n",
    "\n",
    "$$\n",
    "y_j = \\sum_{i=1}^N \\alpha_i \\left(k(x_i, x_j) + \\sigma^2_\\nu \\delta_{ij}\\right)\n",
    "$$\n",
    "\n",
    "Leading to\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{y} &=& (\\mathbf{K} + \\sigma^2_\\nu \\mathbf{I})\\mathbf{a}\\qquad \\textrm{with }[\\mathbf{K}]_{ij} = k(x_i,x_j)\\\\\n",
    "\\Rightarrow \\mathbf{a}&=& \\mathbf{C}^{-1} \\mathbf{y}\\qquad\\qquad\\textrm{with }\\mathbf{C} = \\mathbf{K}+\\sigma^2_\\nu \\mathbf{I} \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "So the final form of the fitted function is\n",
    "\n",
    "$$\n",
    "f(x) = \\mathbf{k}(x)^T \\mathbf{C}^{-1} \\mathbf{y}\\qquad \\textrm{with } [\\mathbf{k}(x)]_i = k(x_i,x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "const sw2=0.3\n",
    "se = ( (x1,x2) -> sw2*exp(-(x1-x2)^2/(2.0*0.5^2)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = length(rOO)\n",
    "K = zeros(N,N)\n",
    "for i=1:N, j=1:N\n",
    "    K[i,j] = se(rOO[i], rOO[j])\n",
    "end\n",
    "\n",
    "function fit(x, nu2)\n",
    "    CinvY = inv(K + diagm(nu2*ones(N))) * fmean\n",
    "    k = zeros(N)\n",
    "    f = zeros(x)\n",
    "    for i=1:length(x)\n",
    "        for j=1:length(k)\n",
    "            k[j] = se(rOO[j], x[i])\n",
    "        end\n",
    "        f[i] = k â‹… CinvY\n",
    "    end\n",
    "    f\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure()\n",
    "\n",
    "errorbar(rOO, fmean, ferr)\n",
    "r = 2.2:0.01:8.0\n",
    "plot(r, fit(r, 0.001), label=L\"\\sigma^2_\\nu = 0.001\")\n",
    "plot(r, fit(r, 0.01), label=L\"\\sigma^2_\\nu = 0.01\")\n",
    "plot(r, fit(r, 0.1), label=L\"\\sigma^2_\\nu = 0.1\")\n",
    "legend(); xlabel(L\"r_\\mathrm{OO}\");ylabel(\"f\")\n",
    "close(gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CinvY = inv(K + diagm(0.01*ones(N))) * fmean # list of coefficients of the basis functions\n",
    "\n",
    "figure()\n",
    "errorbar(rOO, fmean, ferr)\n",
    "r = 2.4:0.01:8\n",
    "for i = 1:length(rOO)\n",
    "    plot(r, 0.1*CinvY[i]*map(x->se(x, rOO[i]), r), \"k-\")\n",
    "end\n",
    "plot(r, fit(r, 0.01), \"r-\")\n",
    "axis([2, 8, -0.5, 0.5])\n",
    "xlabel(L\"r_\\mathrm{OO}\");ylabel(\"f\")\n",
    "close(gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure()\n",
    "f1 = cumsum(-fmean)*0.2\n",
    "plot(rOO, f1-minimum(f1), \"bo-\", label=L\"\\mathrm{cumsum}(\\bar f)\")\n",
    "r = 2.2:0.01:8\n",
    "f2 = cumsum(-fit(r, 0.01))*0.01\n",
    "plot(r, f2-minimum(f2[1:100]), \"r-\", label=L\"\\mathrm{cumsum}(\\mathrm{GP})\")\n",
    "legend(loc=4)\n",
    "xlabel(L\"r_\\mathrm{OO}\"); ylabel(\"A(r)\")\n",
    "close(gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How do we set the correct $\\sigma_\\nu$ ? \n",
    "* How do we get error bars on the interpolation ?\n",
    "* When we integrate the interpolated force, how do we propagate error bars? \n",
    "* How to generalise to multiple dimensions? (particularly the integration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to Gaussian processes\n",
    "\n",
    "We write $f$ again as a linear combination of $H$ basis functions $\\{\\phi_h(x)\\}$, but now with weights $w_h$ which are random variables,\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{h=1}^H w_h \\phi_h(x)\n",
    "$$\n",
    "\n",
    "Substituting in the data points,\n",
    "\n",
    "$$\n",
    "f(x_i) = \\sum_h R_{ih} w_h\\qquad \\textrm{with } R_{ih} = \\phi_h(x_i)\n",
    "$$\n",
    "\n",
    "Let us take our prior the multivariate Gaussian distribution for the coefficients,\n",
    "\n",
    "$$\n",
    "P(\\mathbf{w}) = N(0, \\sigma^2_w \\mathbf{I})\n",
    "$$\n",
    "\n",
    "Since the $f(x_i)$ values are linear combinations of the $w$ values, their distribution is also normal,\n",
    "\n",
    "$$\n",
    "P(\\mathbf{f}) = N(0, \\sigma^2_w \\mathbf{RR}^T)\n",
    "$$\n",
    "\n",
    "The observations $\\mathbf{y}$ differ from $\\mathbf{f}$ by noise $\\varepsilon$ which we also take to be Gaussian,\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{f} + \\varepsilon\\qquad P(\\varepsilon) = N(0, \\sigma^2_\\nu \\mathbf{I})\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\mathbf{y}) = N(0, \\sigma^2_w \\mathbf{RR}^T + \\sigma^2_\\nu\\mathbf{I})\n",
    "$$\n",
    "\n",
    "Note that basis functions $\\phi$ no longer appear, we only need the covariance of the data locations. Taking $\\phi$ to be Gaussian, and $H\\rightarrow \\infty$, we get\n",
    "\n",
    "$$\n",
    "\\textrm{Cov}[f(x_i), f(x_j)] = [\\mathbf{RR}^T]_{ij} = \\sigma^2_w e^{-|x_i-x_j|^2/2\\sigma^2}\n",
    "$$\n",
    "\n",
    "And similarly, \n",
    "\n",
    "$$\n",
    "\\textrm{Cov}[y_i, y_j] =  \\sigma^2_w e^{-|x_i-x_j|^2/2\\sigma^2} + \\sigma^2_\\nu \\delta_{ij} \\equiv C(x_i, x_j) \\equiv [\\mathbf{C}_N]_{ij}\n",
    "$$\n",
    "\n",
    "\n",
    "To predict the next observation, $y_{N+1}$ at $x_{N+1}$, after $N$ observations we consider the conditional\n",
    "\n",
    "$$\n",
    "P(y_{N+1} | \\mathbf{y}_N) = \\frac{P(y_{N+1}, \\mathbf{y})}{P(\\mathbf{y}_N)}\n",
    "$$\n",
    "\n",
    "Dropping the normalisations, we ultimately want the joint probability,\n",
    "\n",
    "$$\n",
    "P(y_{N+1} | \\mathbf{y}_N) \\propto P(y_{N+1}, \\mathbf{y}_N)\n",
    "$$\n",
    "\n",
    "and we would like it as a probability distribution for $y_{N+1}$ in terms of $\\mathbf{y}_N$ and $\\mathbf{x}_N$ as parameters. \n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{y}_N) &=& N(0,\\mathbf{C}_N) \\propto e^{-\\mathbf{y}_N^T \\mathbf{C}^{-1}_N \\mathbf{y}_N}\\\\\n",
    "P(y_{N+1},\\mathbf{y}_N) &=& N(0,\\mathbf{C}_{N+1}) \\propto e^{-[\\mathbf{y}_N^T y_{N+1}] \\mathbf{C}^{-1}_{N+1} [\\mathbf{y}_N y_{N+1}]}\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "The covariance matrices are as follows\n",
    "\n",
    "$$\n",
    "\\mathbf{C}_{N+1} = \\left[\\begin{matrix}\n",
    "\\mathbf{C}_N & \\mathbf{k}\\\\\n",
    "\\mathbf{k}^T & \\kappa\\\\\n",
    "\\end{matrix}\\right] \\qquad [\\mathbf{k}]_i = C(x_i,x_{N+1}), \\kappa = C(x_{N+1},x_{N+1})\n",
    "$$\n",
    "\n",
    "There is a neat trick to expressing $\\mathbf{C}^{-1}_{N+1}$, \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{C}^{-1}_{N+1} &=& \\left[\\begin{matrix}\n",
    "\\mathbf{M} & \\mathbf{m}\\\\\n",
    "\\mathbf{m}^T & \\mu\\\\\n",
    "\\end{matrix}\\right]\\\\\n",
    "\\mathbf{M} &=& \\mathbf{C}^{-1}_{N} + \\frac1\\mu \\mathbf{m}\\mathbf{m}^T\\\\\n",
    "\\mathbf{m} &=& -\\mu \\mathbf{C}^{-1}_{N} \\mathbf{k}\\\\\n",
    "\\mu &=& (\\kappa - \\mathbf{k}^T \\mathbf{C}^{-1}_{N} \\mathbf{k})^{-1}\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Using this, we can write \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "[\\mathbf{y}_N^T y_{N+1}] \\mathbf{C}^{-1}_{N+1} [\\mathbf{y}_N y_{N+1}] &=& \n",
    "\\mathbf{y}^T_N \\mathbf{M}\\mathbf{y}_N + 2\\mathbf{y}^T_N \\mathbf{m} y_{N+1}+\\mu y^2_{N+1}\\\\\n",
    "&=& \\mu(y_{N+1} + \\mathbf{y}^T_N\\mathbf{m}/\\mu)^2 + \\ldots \\qquad\\textrm{(completing the square)}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "So the conditional probability for $y_{N+1}$ is then\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "P(y_{N+1} | \\mathbf{y}_N) &\\propto& e^{-(y_{N+1} - \\bar y)^2/2\\hat\\sigma^2}\\\\\n",
    "\\bar y &=& \\mathbf{k}^T \\mathbf{C}^{-1}_N \\mathbf{y}_N\\\\\n",
    "2\\hat\\sigma^2 &=& \\kappa - \\mathbf{k}^T \\mathbf{C}^{-1}_N \\mathbf{k}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "The highest probability for $y_{N+1}$ occurs at the mean, $\\bar y$, and we thus recovered the linear regression solution. \n",
    "\n",
    "* We now have an errorbar $\\hat\\sigma$\n",
    "* Parameters have physical meaning\n",
    " * $\\sigma_w$ is expected scale of the function\n",
    " * $\\sigma_\\nu$ is noise of observations\n",
    " * $\\sigma$ is covariance length scale of input space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples from the Gaussian prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = (0:0.1:10)\n",
    "Nx=length(x)\n",
    "R = zeros(Nx, Nx)\n",
    "for i=1:Nx, j=1:Nx\n",
    "    R[i,j] = se(x[i],x[j])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure()\n",
    "plot(x, R*randn(Nx))\n",
    "plot(x, R*randn(Nx))\n",
    "plot(x, R*randn(Nx))\n",
    "plot(x, R*randn(Nx))\n",
    "close(gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function fit_err(x, nu2)\n",
    "    Cinv = inv(K + diagm(nu2*ones(N)))\n",
    "    CinvY =  Cinv * fmean\n",
    "    k = zeros(N)\n",
    "    f = zeros(x)\n",
    "    err = zeros(x)\n",
    "    for i=1:length(x)\n",
    "        for j=1:length(k)\n",
    "            k[j] = se(rOO[j], x[i])\n",
    "        end\n",
    "        f[i] = k â‹… CinvY\n",
    "        err[i] = 0.5*(sw2+nu2 - k â‹… (Cinv * k))\n",
    "    end\n",
    "    f,sqrt(err)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure()\n",
    "errorbar(rOO, fmean, ferr, fmt=\"b-\")\n",
    "r = 2:0.01:8\n",
    "f,err = fit_err(r, 0.01)\n",
    "plot(r, f, \"r-\")\n",
    "errorbar(r[1:50:end], f[1:50:end], err[1:50:end], fmt=\"r.\")\n",
    "axis([2, 8, -1, 1])\n",
    "xlabel(L\"r_\\mathrm{OO}\"); ylabel(\"f\")\n",
    "close(gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the potential directly from derivative observations\n",
    "\n",
    "What we would really like is to compute $P(y_{N+1} | L(\\mathbf{y}))$, where the linear operator $L$ could be e.g.  $\\partial/\\partial x$. \n",
    "\n",
    "In the Gaussian process framework, all we need for this is the covariance structure of $L(\\mathbf{y})$, and $\\textrm{Cov}[y_{N+1}, L(\\mathbf{y})]$. \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\textrm{Cov}[y_i, \\partial y_j/\\partial x_j] &=& \\partial \\textrm{Cov}[y_i,y_j]/\\partial x_j\\\\\n",
    "&=& \\frac1{\\sigma^2}(x_i-x_j) e^{-|x_i-x_j|^2/2\\sigma^2}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\textrm{Cov}[\\partial y_i/\\partial x_i, \\partial y_j/\\partial x_j] =\n",
    "-\\frac1{\\sigma^4}(x_i-x_j)^2 e^{-|x_i-x_j|^2/2\\sigma^2}+\\frac1{\\sigma^2}e^{-|x_i-x_j|^2/2\\sigma^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "const sw2=0.1^2\n",
    "\n",
    "se_fd = ( (x1,x2) -> sw2*(x1-x2)*exp(-(x1-x2)^2/(2.0)) )\n",
    "\n",
    "se_dd = ( (x1,x2) -> sw2*(1-(x1-x2)^2)*exp(-(x1-x2)^2/(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Kdd = zeros(N,N)\n",
    "for i=1:N, j=1:N\n",
    "    Kdd[i,j] = se_dd(rOO[i], rOO[j])\n",
    "end\n",
    "\n",
    "function fit_deriv_err(x, nu2)\n",
    "    Cinv = inv(Kdd + diagm(nu2*ones(N)))\n",
    "    CinvY =  Cinv * (-fmean)\n",
    "    k = zeros(N)\n",
    "    f = zeros(x)\n",
    "    err = zeros(x)\n",
    "    for i=1:length(x)\n",
    "        for j=1:length(k)\n",
    "            k[j] = se_fd(x[i], rOO[j])\n",
    "        end\n",
    "        f[i] = k â‹… CinvY\n",
    "        err[i] = 0.5*(sw2+nu2 - k â‹… (Cinv * k))\n",
    "    end\n",
    "    f,sqrt(err-minimum(err))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure()\n",
    "f1 = cumsum(-fmean)*0.2\n",
    "plot(rOO, f1-f1[9], \"bo-\", label=L\"\\mathrm{cumsum}(\\bar f)\")\n",
    "\n",
    "\n",
    "r = 2.2:0.01:8\n",
    "f,err = fit_deriv_err(r, 0.1^2)\n",
    "plot(r, f-f[200], \"r-\", label=\"GP deriv\")\n",
    "errorbar(r[1:50:end], f[1:50:end]-f[200], err[1:50:end], fmt=\"r.\")\n",
    "#axis([2, 8, -1, 1])\n",
    "xlabel(L\"r_\\mathrm{OO}\"); ylabel(\"A(r)\")\n",
    "legend(loc=4)\n",
    "close(gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.3",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
